# CSV íŒŒì¼ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ê¸°
# ì¶”ì¶œëœ ë³´í—˜ ë°ì´í„°ì—ì„œ ê°€ì¥ ë§ì´ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ ë¶„ì„

import pandas as pd
import json
import re
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

class WordFrequencyAnalyzer:
    """CSV íŒŒì¼ì—ì„œ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„"""
    
    def __init__(self, csv_file_path):
        self.csv_file = csv_file_path
        self.df = None
        self.word_analysis = {}
        
    def load_csv(self):
        """CSV íŒŒì¼ ë¡œë“œ"""
        try:
            self.df = pd.read_csv(self.csv_file, encoding='utf-8-sig')
            print(f"âœ… CSV íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(self.df)}ê°œ ìƒí’ˆ")
            return True
        except Exception as e:
            print(f"âŒ CSV íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def analyze_word_frequency(self):
        """ì „ì²´ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„"""
        
        if self.df is None:
            print("âŒ CSV íŒŒì¼ì„ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return
        
        print("\nğŸ“Š ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ì‹œì‘...")
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        all_text = self._collect_all_text()
        
        # ë‹¨ì–´ ë¶„ì„
        self.word_analysis = {
            'korean_words': self._analyze_korean_words(all_text),
            'english_words': self._analyze_english_words(all_text),
            'numbers': self._analyze_numbers(all_text),
            'insurance_terms': self._analyze_insurance_terms(all_text),
            'company_terms': self._analyze_company_terms(all_text)
        }
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_analysis_results()
        
        return self.word_analysis
    
    def _collect_all_text(self):
        """ëª¨ë“  í…ìŠ¤íŠ¸ ë°ì´í„° ìˆ˜ì§‘"""
        all_text = ""
        
        # 1. ê¸°ë³¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ë“¤
        text_columns = ['product_name', 'product_category', 'sales_channel', 
                       'premium_range', 'coverage_scope', 'waiting_period']
        
        for col in text_columns:
            if col in self.df.columns:
                text_data = self.df[col].fillna('').astype(str)
                all_text += " ".join(text_data) + " "
        
        # 2. JSON ì»¬ëŸ¼ë“¤ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        json_columns = ['coverage_details', 'special_coverage', 'emergency_services',
                       'partner_benefits', 'digital_services', 'discount_options',
                       'excluded_items', 'health_requirements', 'documentation_required']
        
        for col in json_columns:
            if col in self.df.columns:
                for json_str in self.df[col].fillna('[]'):
                    try:
                        if isinstance(json_str, str) and json_str.strip():
                            # JSON íŒŒì‹±
                            data = json.loads(json_str)
                            extracted_text = self._extract_text_from_json(data)
                            all_text += extracted_text + " "
                    except:
                        # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        all_text += str(json_str) + " "
        
        return all_text
    
    def _extract_text_from_json(self, data):
        """JSON ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        text = ""
        
        if isinstance(data, dict):
            for key, value in data.items():
                text += str(key) + " " + str(value) + " "
        elif isinstance(data, list):
            for item in data:
                if isinstance(item, dict):
                    text += self._extract_text_from_json(item)
                else:
                    text += str(item) + " "
        else:
            text += str(data) + " "
        
        return text
    
    def _analyze_korean_words(self, text):
        """í•œê¸€ ë‹¨ì–´ ë¶„ì„"""
        # í•œê¸€ 2ê¸€ì ì´ìƒ ì¶”ì¶œ
        korean_words = re.findall(r'[ê°€-í£]{2,}', text)
        
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = {
            'ë³´í—˜', 'ìë™ì°¨', 'ìƒí’ˆ', 'ì„œë¹„ìŠ¤', 'ê³ ê°', 'ê°€ì…', 'ì§€ì›', 'ì œê³µ', 'í¬í•¨', 'ê²½ìš°',
            'ëŒ€ìƒ', 'ê¸°ì¤€', 'ì´ìƒ', 'ì´í•˜', 'ê´€ë ¨', 'í•´ë‹¹', 'ì „ì²´', 'ì¼ë°˜', 'ì‚¬ìš©', 'ê°€ëŠ¥',
            'í•„ìš”', 'í™•ì¸', 'ì‹ ì²­', 'ê³„ì•½', 'ì•½ê´€', 'ì¡°ê±´', 'ë‚´ìš©', 'ì •ë³´', 'ë°©ë²•', 'ì ˆì°¨'
        }
        
        filtered_words = [word for word in korean_words if word not in stop_words]
        word_freq = Counter(filtered_words)
        
        return {
            'total_count': len(korean_words),
            'unique_count': len(set(korean_words)),
            'top_20': dict(word_freq.most_common(20)),
            'all_frequencies': dict(word_freq)
        }
    
    def _analyze_english_words(self, text):
        """ì˜ë¬¸ ë‹¨ì–´ ë¶„ì„"""
        # ì˜ë¬¸ 3ê¸€ì ì´ìƒ ì¶”ì¶œ
        english_words = re.findall(r'[a-zA-Z]{3,}', text.lower())
        
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = {
            'insurance', 'auto', 'car', 'service', 'customer', 'include', 'support',
            'the', 'and', 'for', 'with', 'you', 'are', 'can', 'will', 'from', 'this'
        }
        
        filtered_words = [word for word in english_words if word not in stop_words]
        word_freq = Counter(filtered_words)
        
        return {
            'total_count': len(english_words),
            'unique_count': len(set(english_words)),
            'top_15': dict(word_freq.most_common(15)),
            'all_frequencies': dict(word_freq)
        }
    
    def _analyze_numbers(self, text):
        """ìˆ«ì íŒ¨í„´ ë¶„ì„"""
        # ìˆ«ì ì¶”ì¶œ
        numbers = re.findall(r'\d+', text)
        
        # ì¼ë°˜ì ì¸ ìˆ«ìë“¤ (ì—°ë„, ë‚˜ì´ ë“±) í•„í„°ë§
        filtered_numbers = []
        for num in numbers:
            num_int = int(num)
            # ë³´í—˜ë£Œ ê´€ë ¨ ìˆ«ìë“¤ë§Œ (1000 ì´ìƒ)
            if num_int >= 1000 or (10 <= num_int <= 100):  # ë³´í—˜ë£Œ ë˜ëŠ” ë‚˜ì´/ê¸°ê°„
                filtered_numbers.append(num)
        
        number_freq = Counter(filtered_numbers)
        
        return {
            'total_count': len(numbers),
            'unique_count': len(set(numbers)),
            'top_15': dict(number_freq.most_common(15)),
            'common_ranges': self._categorize_numbers(filtered_numbers)
        }
    
    def _categorize_numbers(self, numbers):
        """ìˆ«ìë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
        categories = {
            'ë‚˜ì´_ê´€ë ¨': [],      # 18-70
            'ë³´í—˜ë£Œ_ê´€ë ¨': [],    # 10000 ì´ìƒ
            'ì—°ë„_ê´€ë ¨': [],      # 2000 ì´ìƒ
            'ê¸°íƒ€_ìˆ«ì': []
        }
        
        for num_str in numbers:
            try:
                num = int(num_str)
                if 18 <= num <= 70:
                    categories['ë‚˜ì´_ê´€ë ¨'].append(num_str)
                elif num >= 10000:
                    categories['ë³´í—˜ë£Œ_ê´€ë ¨'].append(num_str)
                elif 2000 <= num <= 2030:
                    categories['ì—°ë„_ê´€ë ¨'].append(num_str)
                else:
                    categories['ê¸°íƒ€_ìˆ«ì'].append(num_str)
            except:
                categories['ê¸°íƒ€_ìˆ«ì'].append(num_str)
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ ë¹ˆë„
        result = {}
        for category, nums in categories.items():
            if nums:
                freq = Counter(nums)
                result[category] = dict(freq.most_common(5))
        
        return result
    
    def _analyze_insurance_terms(self, text):
        """ë³´í—˜ ì „ë¬¸ìš©ì–´ ë¶„ì„"""
        insurance_terms = [
            'ëŒ€ì¸ë°°ìƒ', 'ëŒ€ë¬¼ë°°ìƒ', 'ìì°¨ë³´í—˜', 'ìê¸°ì‹ ì²´ì‚¬ê³ ', 'ë¬´ë³´í—˜ì°¨ìƒí•´',
            'ì¶œë™ì„œë¹„ìŠ¤', 'ê²¬ì¸ì„œë¹„ìŠ¤', 'ë Œí„°ì¹´', 'í• ì¸', 'íŠ¹ì•½', 'ë©´ì±…',
            'ë‹¤ì´ë ‰íŠ¸', 'ì¢…í•©ë³´í—˜', 'ì±…ì„ë³´í—˜', 'ìš´ì „ìë³´í—˜', 'ìƒí•´ë³´í—˜',
            'ë¬´ì‚¬ê³ í• ì¸', 'ë‹¤ìë…€í• ì¸', 'ë¸”ë™ë°•ìŠ¤í• ì¸', 'í•˜ì´ë¸Œë¦¬ë“œí• ì¸',
            'ê¸´ê¸‰ì¶œë™', 'í˜„ì¥ì¶œë™', 'ëŒ€ë¦¬ìš´ì „', 'ì •ë¹„í• ì¸', 'ì£¼ìœ í• ì¸'
        ]
        
        term_freq = {}
        for term in insurance_terms:
            count = text.count(term)
            if count > 0:
                term_freq[term] = count
        
        return dict(sorted(term_freq.items(), key=lambda x: x[1], reverse=True))
    
    def _analyze_company_terms(self, text):
        """ë³´í—˜ì‚¬ ê´€ë ¨ ìš©ì–´ ë¶„ì„"""
        companies = [
            'ì‚¼ì„±í™”ì¬', 'í˜„ëŒ€í•´ìƒ', 'KBì†í•´ë³´í—˜', 'ë©”ë¦¬ì¸ í™”ì¬', 'DBì†í•´ë³´í—˜',
            'ë¡¯ë°ì†í•´ë³´í—˜', 'í•œí™”ì†í•´ë³´í—˜', 'MGì†í•´ë³´í—˜', 'í¥êµ­í™”ì¬', 'AXAì†í•´ë³´í—˜',
            'í•˜ë‚˜ì†í•´ë³´í—˜', 'ìºë¡¯ì†í•´ë³´í—˜'
        ]
        
        company_freq = {}
        for company in companies:
            # ì •í™•í•œ íšŒì‚¬ëª…ê³¼ ë‹¨ì¶•ëª… ëª¨ë‘ ì²´í¬
            short_name = company.replace('í™”ì¬í•´ìƒë³´í—˜', '').replace('ì†í•´ë³´í—˜', '')
            count = text.count(company) + text.count(short_name)
            if count > 0:
                company_freq[company] = count
        
        return dict(sorted(company_freq.items(), key=lambda x: x[1], reverse=True))
    
    def _print_analysis_results(self):
        """ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ê²°ê³¼")
        print(f"{'='*60}")
        
        # 1. í•œê¸€ ë‹¨ì–´ TOP 20
        print(f"\nğŸ‡°ğŸ‡· í•œê¸€ ë‹¨ì–´ TOP 20:")
        korean_top = self.word_analysis['korean_words']['top_20']
        for i, (word, count) in enumerate(korean_top.items(), 1):
            print(f"  {i:2d}. {word}: {count}íšŒ")
        
        # 2. ì˜ë¬¸ ë‹¨ì–´ TOP 15
        print(f"\nğŸ‡ºğŸ‡¸ ì˜ë¬¸ ë‹¨ì–´ TOP 15:")
        english_top = self.word_analysis['english_words']['top_15']
        for i, (word, count) in enumerate(english_top.items(), 1):
            print(f"  {i:2d}. {word}: {count}íšŒ")
        
        # 3. ë³´í—˜ ì „ë¬¸ìš©ì–´
        print(f"\nğŸ¥ ë³´í—˜ ì „ë¬¸ìš©ì–´ ë¹ˆë„:")
        insurance_terms = self.word_analysis['insurance_terms']
        for i, (term, count) in enumerate(insurance_terms.items(), 1):
            if i <= 10:  # ìƒìœ„ 10ê°œë§Œ
                print(f"  {i:2d}. {term}: {count}íšŒ")
        
        # 4. ìˆ«ì ë¶„ì„
        print(f"\nğŸ”¢ ì£¼ìš” ìˆ«ì TOP 10:")
        numbers_top = self.word_analysis['numbers']['top_15']
        for i, (num, count) in enumerate(list(numbers_top.items())[:10], 1):
            print(f"  {i:2d}. {num}: {count}íšŒ")
        
        # 5. ë³´í—˜ì‚¬ë³„ ì–¸ê¸‰ ë¹ˆë„
        print(f"\nğŸ¢ ë³´í—˜ì‚¬ë³„ ì–¸ê¸‰ ë¹ˆë„:")
        company_terms = self.word_analysis['company_terms']
        for i, (company, count) in enumerate(company_terms.items(), 1):
            if i <= 12:  # ëª¨ë“  ë³´í—˜ì‚¬
                print(f"  {i:2d}. {company}: {count}íšŒ")
        
        # 6. í†µê³„ ìš”ì•½
        print(f"\nğŸ“ˆ í†µê³„ ìš”ì•½:")
        print(f"  ì´ í•œê¸€ ë‹¨ì–´: {self.word_analysis['korean_words']['total_count']:,}ê°œ")
        print(f"  ê³ ìœ  í•œê¸€ ë‹¨ì–´: {self.word_analysis['korean_words']['unique_count']:,}ê°œ")
        print(f"  ì´ ì˜ë¬¸ ë‹¨ì–´: {self.word_analysis['english_words']['total_count']:,}ê°œ")
        print(f"  ê³ ìœ  ì˜ë¬¸ ë‹¨ì–´: {self.word_analysis['english_words']['unique_count']:,}ê°œ")
        print(f"  ë°œê²¬ëœ ë³´í—˜ ì „ë¬¸ìš©ì–´: {len(insurance_terms)}ê°œ")
    
    def save_analysis_to_csv(self, output_prefix='word_analysis'):
        """ë¶„ì„ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥"""
        
        if not self.word_analysis:
            print("âŒ ë¶„ì„ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return
        
        try:
            # 1. í•œê¸€ ë‹¨ì–´ ë¹ˆë„
            korean_df = pd.DataFrame(list(self.word_analysis['korean_words']['all_frequencies'].items()),
                                   columns=['ë‹¨ì–´', 'ë¹ˆë„'])
            korean_df = korean_df.sort_values('ë¹ˆë„', ascending=False)
            korean_df.to_csv(f'{output_prefix}_korean_words.csv', index=False, encoding='utf-8-sig')
            
            # 2. ì˜ë¬¸ ë‹¨ì–´ ë¹ˆë„
            english_df = pd.DataFrame(list(self.word_analysis['english_words']['all_frequencies'].items()),
                                    columns=['Word', 'Frequency'])
            english_df = english_df.sort_values('Frequency', ascending=False)
            english_df.to_csv(f'{output_prefix}_english_words.csv', index=False, encoding='utf-8-sig')
            
            # 3. ë³´í—˜ ì „ë¬¸ìš©ì–´
            if self.word_analysis['insurance_terms']:
                terms_df = pd.DataFrame(list(self.word_analysis['insurance_terms'].items()),
                                      columns=['ë³´í—˜ìš©ì–´', 'ë¹ˆë„'])
                terms_df.to_csv(f'{output_prefix}_insurance_terms.csv', index=False, encoding='utf-8-sig')
            
            # 4. ì¢…í•© ìš”ì•½
            summary_data = {
                'ë¶„ì„_í•­ëª©': ['ì´ í•œê¸€ë‹¨ì–´', 'ê³ ìœ  í•œê¸€ë‹¨ì–´', 'ì´ ì˜ë¬¸ë‹¨ì–´', 'ê³ ìœ  ì˜ë¬¸ë‹¨ì–´', 'ë³´í—˜ì „ë¬¸ìš©ì–´'],
                'ê°œìˆ˜': [
                    self.word_analysis['korean_words']['total_count'],
                    self.word_analysis['korean_words']['unique_count'],
                    self.word_analysis['english_words']['total_count'],
                    self.word_analysis['english_words']['unique_count'],
                    len(self.word_analysis['insurance_terms'])
                ]
            }
            
            summary_df = pd.DataFrame(summary_data)
            summary_df.to_csv(f'{output_prefix}_summary.csv', index=False, encoding='utf-8-sig')
            
            print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
            print(f"  - {output_prefix}_korean_words.csv")
            print(f"  - {output_prefix}_english_words.csv")
            print(f"  - {output_prefix}_insurance_terms.csv")
            print(f"  - {output_prefix}_summary.csv")
            
        except Exception as e:
            print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

# ì‚¬ìš© ì˜ˆì‹œ
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("=== CSV íŒŒì¼ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ê¸° ===")
    
    # CSV íŒŒì¼ ê²½ë¡œ (ìˆ˜ì • í•„ìš”)
    csv_file_path = "detailed_insurance_products.csv"  # â† ì—¬ê¸°ë¥¼ ì‹¤ì œ CSV íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = WordFrequencyAnalyzer(csv_file_path)
    
    # CSV ë¡œë“œ
    if not analyzer.load_csv():
        print("ğŸ’¡ CSV íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ì‹¤í–‰
    word_analysis = analyzer.analyze_word_frequency()
    
    # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
    analyzer.save_analysis_to_csv('insurance_word_analysis')
    
    print(f"\nğŸ‰ ë¶„ì„ ì™„ë£Œ!")
    print(f"ê²°ê³¼ íŒŒì¼ë“¤ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
print("\nğŸ“¦ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬:")
print("pip install pandas matplotlib seaborn")