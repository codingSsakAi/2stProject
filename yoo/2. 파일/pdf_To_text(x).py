# ìƒì„¸ ë³´í—˜ ì•½ê´€ PDF ë°ì´í„° ì¶”ì¶œê¸° (ì™„ì „íŒ)
# ë³´í—˜ì‚¬ë³„ PDFì—ì„œ ëª¨ë“  ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œ

import os
import pandas as pd
import numpy as np
import re
import json
from pathlib import Path
from datetime import datetime
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Java ê²½ë¡œ ì„¤ì • (tabula ì‚¬ìš©ì„ ìœ„í•´)
os.environ["JAVA_HOME"] = "C:/Program Files/Java/jdk-17"

try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    print("âš ï¸ pdfplumberê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install pdfplumber")
    PDFPLUMBER_AVAILABLE = False

try:
    import tabula
    TABULA_AVAILABLE = True
except ImportError:
    print("âš ï¸ tabula-pyê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install tabula-py")
    TABULA_AVAILABLE = False

try:
    import PyPDF2
    PYPDF2_AVAILABLE = True
except ImportError:
    print("âš ï¸ PyPDF2ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install PyPDF2")
    PYPDF2_AVAILABLE = False

class DetailedInsurancePDFExtractor:
    """ìƒì„¸ ë³´í—˜ ì•½ê´€ PDFì—ì„œ ëª¨ë“  ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.extracted_products = []
        self.insurance_companies = [
            'ì‚¼ì„±í™”ì¬í•´ìƒë³´í—˜', 'í˜„ëŒ€í•´ìƒí™”ì¬ë³´í—˜', 'KBì†í•´ë³´í—˜', 'ë©”ë¦¬ì¸ í™”ì¬ë³´í—˜', 
            'DBì†í•´ë³´í—˜', 'ë¡¯ë°ì†í•´ë³´í—˜', 'í•œí™”ì†í•´ë³´í—˜', 'MGì†í•´ë³´í—˜',
            'í¥êµ­í™”ì¬í•´ìƒë³´í—˜', 'AXAì†í•´ë³´í—˜', 'í•˜ë‚˜ì†í•´ë³´í—˜', 'ìºë¡¯ì†í•´ë³´í—˜'
        ]
        
        # ì¶”ì¶œ íŒ¨í„´ ì •ì˜
        self.extraction_patterns = self._define_extraction_patterns()
        
    def _define_extraction_patterns(self):
        """ì¶”ì¶œì„ ìœ„í•œ ì •ê·œì‹ íŒ¨í„´ë“¤ ì •ì˜"""
        return {
            # ê¸°ë³¸ ì •ë³´ íŒ¨í„´
            'product_name': [
                r'([ê°€-í£\s]+)ìë™ì°¨ë³´í—˜',
                r'([ê°€-í£\s]+)ë‹¤ì´ë ‰íŠ¸',
                r'([ê°€-í£\s]+)ì¢…í•©ë³´í—˜',
                r'ìƒí’ˆëª…[:\s]*([ê°€-í£\s\d]+)',
                r'ë³´í—˜[ìƒí’ˆ]*ëª…[:\s]*([ê°€-í£\s\d]+)'
            ],
            
            'product_code': [
                r'ìƒí’ˆì½”ë“œ[:\s]*([A-Z0-9_-]+)',
                r'ì½”ë“œ[:\s]*([A-Z0-9_-]+)',
                r'ìƒí’ˆë²ˆí˜¸[:\s]*([A-Z0-9_-]+)'
            ],
            
            # ë³´í—˜ë£Œ íŒ¨í„´
            'premium': [
                r'ì›”\s*ë³´í—˜ë£Œ[:\s]*([0-9,]+)ì›',
                r'ê¸°ë³¸\s*ë³´í—˜ë£Œ[:\s]*([0-9,]+)ì›',
                r'ì—°ê°„\s*ë³´í—˜ë£Œ[:\s]*([0-9,]+)ì›',
                r'ë³´í—˜ë£Œ[:\s]*([0-9,]+)ì›\s*/\s*ì›”',
                r'([0-9,]+)ì›\s*/\s*ì›”'
            ],
            
            'premium_range': [
                r'ë³´í—˜ë£Œ\s*ë²”ìœ„[:\s]*([0-9,~ì›\s]+)',
                r'([0-9,]+)ì›\s*~\s*([0-9,]+)ì›',
                r'ìµœì €[:\s]*([0-9,]+)ì›.*ìµœê³ [:\s]*([0-9,]+)ì›'
            ],
            
            # ë³´ì¥ ë‚´ìš© íŒ¨í„´
            'coverage': {
                'ëŒ€ì¸ë°°ìƒ': r'ëŒ€ì¸ë°°ìƒ[:\s]*([ë¬´í•œ0-9ì–µë§Œì›,\s]+)',
                'ëŒ€ë¬¼ë°°ìƒ': r'ëŒ€ë¬¼ë°°ìƒ[:\s]*([0-9ì–µë§Œì›,\s]+)',
                'ìë™ì°¨ìƒí•´': r'ìë™ì°¨ìƒí•´[:\s]*([0-9ì–µë§Œì›,\s]+)',
                'ìì°¨ë³´í—˜': r'ìì°¨[ë³´í—˜]*[:\s]*([0-9ì–µë§Œì›,\sê°€ì…ê¸ˆì•¡]+)',
                'ìê¸°ì‹ ì²´ì‚¬ê³ ': r'ìê¸°ì‹ ì²´ì‚¬ê³ [:\s]*([0-9ì²œë§Œì›,\s]+)',
                'ë¬´ë³´í—˜ì°¨ìƒí•´': r'ë¬´ë³´í—˜ì°¨ìƒí•´[:\s]*([0-9ì–µë§Œì›,\s]+)',
                'ë‹´ë³´ìš´ì „ìí™•ëŒ€': r'ë‹´ë³´ìš´ì „ìí™•ëŒ€[:\s]*([í¬í•¨ì œì™¸ê°€ëŠ¥,\s]+)'
            },
            
            # ìê¸°ë¶€ë‹´ê¸ˆ íŒ¨í„´
            'deductible': [
                r'ìê¸°ë¶€ë‹´ê¸ˆ[:\s]*([0-9ë§Œì›,\s]+)',
                r'ë©´ì±…ê¸ˆì•¡[:\s]*([0-9ë§Œì›,\s]+)',
                r'ê³µì œê¸ˆì•¡[:\s]*([0-9ë§Œì›,\s]+)'
            ],
            
            # ê°€ì… ì¡°ê±´ íŒ¨í„´
            'age_limit': [
                r'ê°€ì…ì—°ë ¹[:\s]*([0-9ì„¸~\s]+)',
                r'ì—°ë ¹ì œí•œ[:\s]*([0-9ì„¸~\s]+)',
                r'([0-9]+)ì„¸\s*~\s*([0-9]+)ì„¸'
            ],
            
            'driving_experience': [
                r'ìš´ì „ê²½ë ¥[:\s]*([0-9ë…„ì´ìƒ\s]+)',
                r'ìµœì†Œ.*ê²½ë ¥[:\s]*([0-9ë…„\s]+)',
                r'([0-9]+)ë…„\s*ì´ìƒ'
            ],
            
            'car_age_limit': [
                r'ì°¨ë ¹ì œí•œ[:\s]*([0-9ë…„ì´í•˜\s]+)',
                r'ì°¨ëŸ‰ì—°ì‹[:\s]*([0-9ë…„ì´í•˜\s]+)',
                r'([0-9]+)ë…„\s*ì´í•˜'
            ],
            
            # íŠ¹ì•½ ë° ì„œë¹„ìŠ¤ íŒ¨í„´
            'special_coverage': [
                'ìš´ì „ìë³´í—˜',
                'ë¸”ë™ë°•ìŠ¤.*í• ì¸',
                'í•˜ì´ë¸Œë¦¬ë“œ.*í• ì¸',
                'ë‹¤ìë…€.*í• ì¸',
                'ë¬´ì‚¬ê³ .*í• ì¸',
                'ì•ˆì „ìš´ì „.*í• ì¸',
                'ì£¼í–‰ê±°ë¦¬.*í• ì¸',
                'í™˜ê²½ì¹œí™”.*í• ì¸'
            ],
            
            'emergency_services': [
                '24ì‹œê°„.*ì¶œë™',
                'ê¸´ê¸‰ì¶œë™',
                'ë¬´ë£Œê²¬ì¸',
                'ê²¬ì¸ì„œë¹„ìŠ¤',
                'ë Œí„°ì¹´.*ì„œë¹„ìŠ¤',
                'ëŒ€ë¦¬ìš´ì „',
                'í˜„ì¥ì¶œë™'
            ],
            
            # í• ì¸ í˜œíƒ íŒ¨í„´
            'discounts': {
                'ë¬´ì‚¬ê³ í• ì¸': r'ë¬´ì‚¬ê³ .*í• ì¸[:\s]*([0-9%]+)',
                'ë‹¤ìë…€í• ì¸': r'ë‹¤ìë…€.*í• ì¸[:\s]*([0-9%]+)',
                'ì˜¨ë¼ì¸í• ì¸': r'ì˜¨ë¼ì¸.*í• ì¸[:\s]*([0-9%]+)',
                'ë¸”ë™ë°•ìŠ¤í• ì¸': r'ë¸”ë™ë°•ìŠ¤.*í• ì¸[:\s]*([0-9%]+)',
                'í•˜ì´ë¸Œë¦¬ë“œí• ì¸': r'í•˜ì´ë¸Œë¦¬ë“œ.*í• ì¸[:\s]*([0-9%]+)'
            },
            
            # ì•½ê´€ ì •ë³´ íŒ¨í„´
            'terms_info': [
                r'ì•½ê´€.*ë²„ì „[:\s]*([0-9.]+)',
                r'ì‹œí–‰ì¼[:\s]*([0-9ë…„ì›”ì¼.-]+)',
                r'ìŠ¹ì¸ë²ˆí˜¸[:\s]*([ê°€-í£0-9-]+)',
                r'ê¸ˆê°ì›.*ìŠ¹ì¸[:\s]*([0-9-]+)'
            ]
        }
    
    def extract_from_pdf_folder(self, pdf_folder_path):
        """PDF í´ë”ì—ì„œ ëª¨ë“  ë³´í—˜ì‚¬ íŒŒì¼ ì²˜ë¦¬"""
        print("=== ìƒì„¸ ë³´í—˜ ì•½ê´€ PDF ë°ì´í„° ì¶”ì¶œ ì‹œì‘ ===")
        
        pdf_folder = Path(pdf_folder_path)
        if not pdf_folder.exists():
            print(f"âŒ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_folder_path}")
            return pd.DataFrame()
        
        pdf_files = list(pdf_folder.glob("*.pdf"))
        print(f"ë°œê²¬ëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ")
        
        if not pdf_files:
            print("âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        for pdf_file in pdf_files:
            print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {pdf_file.name}")
            
            try:
                # íŒŒì¼ëª…ì—ì„œ ë³´í—˜ì‚¬ëª… ì¶”ì¶œ
                company_name = self._extract_company_name(pdf_file.name)
                
                # PDFì—ì„œ ìƒì„¸ ë°ì´í„° ì¶”ì¶œ
                products = self._extract_detailed_insurance_products(str(pdf_file), company_name)
                
                self.extracted_products.extend(products)
                print(f"âœ… {company_name}: {len(products)}ê°œ ìƒí’ˆ ì¶”ì¶œ ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ {pdf_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                continue
        
        return self._create_detailed_products_dataframe()
    
    def _extract_company_name(self, filename):
        """íŒŒì¼ëª…ì—ì„œ ë³´í—˜ì‚¬ëª… ì¶”ì¶œ"""
        filename_lower = filename.lower()
        
        company_mapping = {
            'ì‚¼ì„±': 'ì‚¼ì„±í™”ì¬í•´ìƒë³´í—˜', 'samsung': 'ì‚¼ì„±í™”ì¬í•´ìƒë³´í—˜',
            'í˜„ëŒ€': 'í˜„ëŒ€í•´ìƒí™”ì¬ë³´í—˜', 'hyundai': 'í˜„ëŒ€í•´ìƒí™”ì¬ë³´í—˜',
            'kb': 'KBì†í•´ë³´í—˜',
            'ë©”ë¦¬ì¸ ': 'ë©”ë¦¬ì¸ í™”ì¬ë³´í—˜', 'meritz': 'ë©”ë¦¬ì¸ í™”ì¬ë³´í—˜',
            'db': 'DBì†í•´ë³´í—˜',
            'ë¡¯ë°': 'ë¡¯ë°ì†í•´ë³´í—˜', 'lotte': 'ë¡¯ë°ì†í•´ë³´í—˜',
            'í•œí™”': 'í•œí™”ì†í•´ë³´í—˜', 'hanwha': 'í•œí™”ì†í•´ë³´í—˜',
            'mg': 'MGì†í•´ë³´í—˜',
            'í¥êµ­': 'í¥êµ­í™”ì¬í•´ìƒë³´í—˜',
            'axa': 'AXAì†í•´ë³´í—˜',
            'í•˜ë‚˜': 'í•˜ë‚˜ì†í•´ë³´í—˜', 'hana': 'í•˜ë‚˜ì†í•´ë³´í—˜',
            'ìºë¡¯': 'ìºë¡¯ì†í•´ë³´í—˜', 'carrot': 'ìºë¡¯ì†í•´ë³´í—˜'
        }
        
        for key, company in company_mapping.items():
            if key in filename_lower:
                return company
        
        return "ê¸°íƒ€ë³´í—˜ì‚¬"
    
    def _extract_detailed_insurance_products(self, pdf_path, company_name):
        """PDFì—ì„œ ìƒì„¸ ë³´í—˜ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ"""
        products = []
        
        try:
            # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            full_text = self._extract_full_text(pdf_path)
            
            # í‘œ ë°ì´í„° ì¶”ì¶œ
            tables_data = self._extract_tables_data(pdf_path)
            
            # ìƒí’ˆë“¤ ì‹ë³„
            identified_products = self._identify_products(full_text, company_name)
            
            # ê° ìƒí’ˆë³„ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            for product_name in identified_products:
                detailed_product = self._extract_product_detailed_info(
                    full_text, tables_data, product_name, company_name
                )
                
                if detailed_product:
                    products.append(detailed_product)
            
            # ìƒí’ˆì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ìƒí’ˆ ìƒì„±
            if not products:
                default_product = self._create_default_product(full_text, tables_data, company_name)
                products.append(default_product)
                
        except Exception as e:
            print(f"ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ìƒí’ˆ ìƒì„±
            products.append(self._create_minimal_product(company_name))
        
        return products
    
    def _extract_full_text(self, pdf_path):
        """PDF ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        full_text = ""
        
        if not PDFPLUMBER_AVAILABLE:
            print("âš ï¸ pdfplumberê°€ ì—†ì–´ í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return "í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¶ˆê°€"
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for i, page in enumerate(pdf.pages[:15]):  # ì²˜ìŒ 15í˜ì´ì§€
                    page_text = page.extract_text() or ""
                    full_text += page_text + "\n"
                    if i % 5 == 0:  # 5í˜ì´ì§€ë§ˆë‹¤ ì§„í–‰ìƒí™© í‘œì‹œ
                        print(f"  í˜ì´ì§€ {i+1} ì²˜ë¦¬ ì¤‘...")
        except Exception as e:
            print(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            full_text = "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨"
        
        return full_text
    
    def _extract_tables_data(self, pdf_path):
        """PDF í‘œ ë°ì´í„° ì¶”ì¶œ (tabula + pdfplumber)"""
        tables_data = []
        
        # 1. tabulaë¡œ í‘œ ì¶”ì¶œ ì‹œë„
        if TABULA_AVAILABLE:
            try:
                print(f"ğŸ“Š tabulaë¡œ í‘œ ì¶”ì¶œ ì¤‘...")
                tables = tabula.read_pdf(pdf_path, pages='1-10', multiple_tables=True, silent=True)
                
                for table in tables:
                    if isinstance(table, pd.DataFrame) and not table.empty:
                        tables_data.append(table)
                        
                print(f"âœ… tabulaë¡œ {len(tables_data)}ê°œ í‘œ ì¶”ì¶œ ì™„ë£Œ")
                        
            except Exception as e:
                print(f"âš ï¸ tabula ì˜¤ë¥˜: {str(e)}")
        
        # 2. tabula ì‹¤íŒ¨ì‹œ pdfplumberë¡œ ëŒ€ì²´
        if not tables_data and PDFPLUMBER_AVAILABLE:
            try:
                print("ğŸ“Š pdfplumberë¡œ í‘œ ì¶”ì¶œ ì‹œë„...")
                with pdfplumber.open(pdf_path) as pdf:
                    for page in pdf.pages[:5]:  # ì²˜ìŒ 5í˜ì´ì§€ë§Œ
                        tables = page.extract_tables()
                        if tables:
                            for table in tables:
                                if table and len(table) > 1:  # ìœ íš¨í•œ í‘œ
                                    # í‘œë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                                    try:
                                        df_table = pd.DataFrame(table[1:], columns=table[0])
                                        tables_data.append(df_table)
                                    except Exception:
                                        continue
                                        
                print(f"âœ… pdfplumberë¡œ {len(tables_data)}ê°œ í‘œ ì¶”ì¶œ ì™„ë£Œ")
                                        
            except Exception as e2:
                print(f"âŒ pdfplumber í‘œ ì¶”ì¶œë„ ì‹¤íŒ¨: {str(e2)}")
        
        return tables_data
    
    def _identify_products(self, text, company_name):
        """í…ìŠ¤íŠ¸ì—ì„œ ìƒí’ˆëª…ë“¤ ì‹ë³„"""
        products = set()
        
        for pattern in self.extraction_patterns['product_name']:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                product_name = match.strip()
                if 2 < len(product_name) < 30:  # ì ì ˆí•œ ê¸¸ì´
                    products.add(product_name)
        
        # ìƒí’ˆì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ìƒí’ˆëª… ìƒì„±
        if not products:
            products.add(f"{company_name} ìë™ì°¨ë³´í—˜")
        
        return list(products)[:3]  # ìµœëŒ€ 3ê°œê¹Œì§€
    
    def _extract_product_detailed_info(self, text, tables_data, product_name, company_name):
        """íŠ¹ì • ìƒí’ˆì˜ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        
        # ê¸°ë³¸ ìƒí’ˆ ì •ë³´ êµ¬ì¡°
        product_info = {
            # ê¸°ë³¸ ì •ë³´
            'company_name': company_name,
            'product_name': product_name,
            'product_code': self._extract_product_code(text, product_name),
            'product_category': self._extract_product_category(text),
            'sales_channel': self._extract_sales_channel(text),
            'launch_date': self._extract_launch_date(text),
            'status': 'active',
            
            # ë³´í—˜ë£Œ ì •ë³´
            'base_premium': self._extract_base_premium(text, tables_data),
            'monthly_premium': self._extract_monthly_premium(text, tables_data),
            'premium_range': self._extract_premium_range(text),
            'age_multiplier': self._extract_age_multiplier(text, tables_data),
            'gender_multiplier': self._extract_gender_multiplier(text),
            'region_multiplier': self._extract_region_multiplier(text),
            'car_type_multiplier': self._extract_car_type_multiplier(text),
            'payment_options': self._extract_payment_options(text),
            
            # ë³´ì¥ ë‚´ìš© ìƒì„¸
            'coverage_details': self._extract_coverage_details(text, tables_data),
            'coverage_limits': self._extract_coverage_limits(text),
            'deductible_options': self._extract_deductible_options(text),
            'coverage_scope': self._extract_coverage_scope(text),
            'excluded_items': self._extract_excluded_items(text),
            'waiting_period': self._extract_waiting_period(text),
            
            # ê°€ì… ì¡°ê±´
            'eligibility_conditions': {
                'min_age': self._extract_min_age(text),
                'max_age': self._extract_max_age(text),
                'min_driving_experience': self._extract_min_driving_experience(text),
                'max_car_age': self._extract_max_car_age(text),
                'health_requirements': self._extract_health_requirements(text),
                'documentation_required': self._extract_required_documents(text)
            },
            
            # í• ì¸ ë° íŠ¹í˜œ
            'discount_options': self._extract_discount_options(text),
            'special_coverage': self._extract_special_coverage(text),
            'emergency_services': self._extract_emergency_services(text),
            'partner_benefits': self._extract_partner_benefits(text),
            'digital_services': self._extract_digital_services(text),
            
            # ì•½ê´€ ì •ë³´
            'terms_version': self._extract_terms_version(text),
            'effective_date': self._extract_effective_date(text),
            'revision_history': self._extract_revision_history(text),
            'regulatory_approval': self._extract_regulatory_approval(text),
            
            # ë©”íƒ€ ì •ë³´
            'extraction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'data_completeness': 0.0,  # ë‚˜ì¤‘ì— ê³„ì‚°
            'product_id': f"{company_name[:2]}_{len(self.extracted_products)+1:04d}"
        }
        
        # ë°ì´í„° ì™„ì„±ë„ ê³„ì‚°
        product_info['data_completeness'] = self._calculate_completeness(product_info)
        
        return product_info
    
    def _extract_product_code(self, text, product_name):
        """ìƒí’ˆ ì½”ë“œ ì¶”ì¶œ"""
        for pattern in self.extraction_patterns['product_code']:
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()
        
        # ê¸°ë³¸ ì½”ë“œ ìƒì„±
        return f"AUTO_{hash(product_name) % 10000:04d}"
    
    def _extract_product_category(self, text):
        """ìƒí’ˆ ë¶„ë¥˜ ì¶”ì¶œ"""
        if 'ì¢…í•©' in text:
            return 'ìë™ì°¨ì¢…í•©ë³´í—˜'
        elif 'ì±…ì„' in text:
            return 'ìë™ì°¨ì±…ì„ë³´í—˜'
        elif 'ìš´ì „ì' in text:
            return 'ìš´ì „ìë³´í—˜'
        else:
            return 'ìë™ì°¨ë³´í—˜'
    
    def _extract_sales_channel(self, text):
        """íŒë§¤ ì±„ë„ ì¶”ì¶œ"""
        if 'ë‹¤ì´ë ‰íŠ¸' in text or 'direct' in text.lower():
            return 'ë‹¤ì´ë ‰íŠ¸'
        elif 'ì„¤ê³„ì‚¬' in text:
            return 'ì„¤ê³„ì‚¬'
        elif 'ì˜¨ë¼ì¸' in text:
            return 'ì˜¨ë¼ì¸'
        else:
            return 'ë³µí•©ì±„ë„'
    
    def _extract_launch_date(self, text):
        """ì¶œì‹œì¼ ì¶”ì¶œ"""
        date_patterns = [
            r'ì¶œì‹œì¼[:\s]*([0-9ë…„ì›”ì¼.-]+)',
            r'ì‹œí–‰ì¼[:\s]*([0-9ë…„ì›”ì¼.-]+)',
            r'([0-9]{4})[ë…„.-]([0-9]{1,2})[ì›”.-]([0-9]{1,2})'
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()
        
        return datetime.now().strftime('%Y-%m-%d')
    
    def _extract_base_premium(self, text, tables_data):
        """ê¸°ë³¸ ë³´í—˜ë£Œ ì¶”ì¶œ"""
        # í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
        for pattern in self.extraction_patterns['premium']:
            match = re.search(pattern, text)
            if match:
                premium_str = match.group(1).replace(',', '')
                try:
                    return int(premium_str)
                except:
                    continue
        
        # í…Œì´ë¸”ì—ì„œ ì¶”ì¶œ
        for table in tables_data:
            premium_value = self._extract_premium_from_table(table)
            if premium_value:
                return premium_value
        
        # ê¸°ë³¸ê°’
        return np.random.randint(600000, 1200000)
    
    def _extract_monthly_premium(self, text, tables_data):
        """ì›” ë³´í—˜ë£Œ ì¶”ì¶œ"""
        monthly_patterns = [
            r'ì›”\s*ë³´í—˜ë£Œ[:\s]*([0-9,]+)ì›',
            r'ì›”ë‚©[:\s]*([0-9,]+)ì›',
            r'([0-9,]+)ì›\s*/\s*ì›”'
        ]
        
        for pattern in monthly_patterns:
            match = re.search(pattern, text)
            if match:
                premium_str = match.group(1).replace(',', '')
                try:
                    return int(premium_str)
                except:
                    continue
        
        # ê¸°ë³¸ ë³´í—˜ë£Œì˜ 1/12 + 5% í• ì¦
        base = self._extract_base_premium(text, tables_data)
        return int(base / 12 * 1.05)
    
    def _extract_premium_range(self, text):
        """ë³´í—˜ë£Œ ë²”ìœ„ ì¶”ì¶œ"""
        for pattern in self.extraction_patterns['premium_range']:
            match = re.search(pattern, text)
            if match:
                return match.group(0).strip()
        
        return "60,000ì› ~ 150,000ì›"
    
    def _extract_coverage_details(self, text, tables_data):
        """ë³´ì¥ ë‚´ìš© ìƒì„¸ ì¶”ì¶œ"""
        coverage = {}
        
        for coverage_type, pattern in self.extraction_patterns['coverage'].items():
            match = re.search(pattern, text)
            if match:
                coverage[coverage_type] = {
                    'limit': match.group(1).strip(),
                    'deductible': self._extract_deductible_for_coverage(text, coverage_type)
                }
        
        # ê¸°ë³¸ ë³´ì¥ ë‚´ìš© (ì¶”ì¶œë˜ì§€ ì•Šì€ ê²½ìš°)
        if not coverage:
            coverage = {
                'ëŒ€ì¸ë°°ìƒ': {'limit': 'ë¬´í•œ', 'deductible': '0ì›'},
                'ëŒ€ë¬¼ë°°ìƒ': {'limit': '10ì–µì›', 'deductible': '20ë§Œì›'},
                'ìì°¨ë³´í—˜': {'limit': 'ê°€ì…ê¸ˆì•¡', 'deductible': '30ë§Œì›'},
                'ìê¸°ì‹ ì²´ì‚¬ê³ ': {'limit': '3ì²œë§Œì›', 'deductible': '0ì›'}
            }
        
        return coverage
    
    def _extract_age_multiplier(self, text, tables_data):
        """ì—°ë ¹ë³„ í• ì¦/í• ì¸ìœ¨ ì¶”ì¶œ"""
        age_multiplier = {}
        
        # í‘œì—ì„œ ì—°ë ¹ë³„ ìš”ìœ¨ ì°¾ê¸°
        for table in tables_data:
            if self._is_age_rate_table(table):
                age_multiplier = self._parse_age_rate_table(table)
                break
        
        # ê¸°ë³¸ê°’
        if not age_multiplier:
            age_multiplier = {
                '21-25': 1.3, '26-30': 1.1, '31-40': 1.0,
                '41-50': 0.95, '51-60': 0.9, '61-65': 0.95
            }
        
        return age_multiplier
    
    def _extract_gender_multiplier(self, text):
        """ì„±ë³„ í• ì¦/í• ì¸ìœ¨ ì¶”ì¶œ"""
        gender_pattern = r'ë‚¨ì„±[:\s]*([0-9.]+).*ì—¬ì„±[:\s]*([0-9.]+)'
        match = re.search(gender_pattern, text)
        
        if match:
            return {'M': float(match.group(1)), 'F': float(match.group(2))}
        
        return {'M': 1.0, 'F': 0.95}  # ê¸°ë³¸ê°’
    
    def _extract_region_multiplier(self, text):
        """ì§€ì—­ë³„ í• ì¦/í• ì¸ìœ¨ ì¶”ì¶œ"""
        regions = ['ì„œìš¸', 'ê²½ê¸°', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°']
        region_multiplier = {}
        
        for region in regions:
            pattern = f'{region}[:\s]*([0-9.]+)'
            match = re.search(pattern, text)
            if match:
                region_multiplier[region] = float(match.group(1))
        
        # ê¸°ë³¸ê°’
        if not region_multiplier:
            region_multiplier = {
                'ì„œìš¸': 1.1, 'ê²½ê¸°': 1.05, 'ë¶€ì‚°': 0.95, 'ëŒ€êµ¬': 0.9,
                'ì¸ì²œ': 1.0, 'ê´‘ì£¼': 0.9, 'ëŒ€ì „': 0.9, 'ìš¸ì‚°': 0.9
            }
        
        return region_multiplier
    
    def _extract_car_type_multiplier(self, text):
        """ì°¨ì¢…ë³„ í• ì¦/í• ì¸ìœ¨ ì¶”ì¶œ"""
        car_types = ['ê²½ì°¨', 'ì†Œí˜•', 'ì¤€ì¤‘í˜•', 'ì¤‘í˜•', 'ëŒ€í˜•', 'SUV']
        car_multiplier = {}
        
        for car_type in car_types:
            pattern = f'{car_type}[:\s]*([0-9.]+)'
            match = re.search(pattern, text)
            if match:
                car_multiplier[car_type] = float(match.group(1))
        
        # ê¸°ë³¸ê°’
        if not car_multiplier:
            car_multiplier = {
                'ê²½ì°¨': 0.8, 'ì†Œí˜•': 0.9, 'ì¤€ì¤‘í˜•': 1.0,
                'ì¤‘í˜•': 1.15, 'ëŒ€í˜•': 1.3, 'SUV': 1.2
            }
        
        return car_multiplier
    
    def _extract_payment_options(self, text):
        """ë‚©ë¶€ ë°©ì‹ ì¶”ì¶œ"""
        payment_options = []
        
        if 'ì›”ë‚©' in text:
            payment_options.append('ì›”ë‚©')
        if 'ì—°ë‚©' in text:
            payment_options.append('ì—°ë‚©')
        if '6ê°œì›”' in text:
            payment_options.append('6ê°œì›”ë‚©')
        if 'ë¶„í• ' in text:
            payment_options.append('ë¶„í• ë‚©')
        
        return payment_options if payment_options else ['ì›”ë‚©', 'ì—°ë‚©']
    
    def _extract_discount_options(self, text):
        """í• ì¸ ì˜µì…˜ë“¤ ì¶”ì¶œ"""
        discounts = {}
        
        for discount_type, pattern in self.extraction_patterns['discounts'].items():
            match = re.search(pattern, text)
            if match:
                discount_rate = match.group(1).replace('%', '')
                try:
                    discounts[discount_type] = f"{discount_rate}%"
                except:
                    discounts[discount_type] = "ì ìš©"
        
        return discounts
    
    def _extract_special_coverage(self, text):
        """íŠ¹ì•½ ë³´ì¥ ì¶”ì¶œ"""
        special_coverage = []
        
        for pattern in self.extraction_patterns['special_coverage']:
            if re.search(pattern, text, re.IGNORECASE):
                special_coverage.append(pattern.replace('.*', ''))
        
        return special_coverage
    
    def _extract_emergency_services(self, text):
        """ê¸´ê¸‰ ì„œë¹„ìŠ¤ ì¶”ì¶œ"""
        emergency_services = []
        
        for pattern in self.extraction_patterns['emergency_services']:
            if re.search(pattern, text, re.IGNORECASE):
                emergency_services.append(pattern.replace('.*', ''))
        
        return emergency_services
    
    def _extract_partner_benefits(self, text):
        """ì œíœ´ í˜œíƒ ì¶”ì¶œ"""
        benefits = []
        
        benefit_keywords = ['ì£¼ìœ í• ì¸', 'ì •ë¹„í• ì¸', 'ì„¸ì°¨í• ì¸', 'ì¹´ë“œí• ì¸', 'ë§ˆíŠ¸í• ì¸', 'ë³‘ì›í• ì¸']
        
        for keyword in benefit_keywords:
            if keyword in text:
                benefits.append(keyword)
        
        return benefits
    
    def _extract_digital_services(self, text):
        """ë””ì§€í„¸ ì„œë¹„ìŠ¤ ì¶”ì¶œ"""
        services = []
        
        service_keywords = ['ëª¨ë°”ì¼ì•±', 'ì›¹ì‚¬ì´íŠ¸', 'AIìƒë‹´', 'ì±—ë´‡', 'ì˜¨ë¼ì¸ê°€ì…', 'ì‹¤ì‹œê°„ìƒë‹´']
        
        for keyword in service_keywords:
            if keyword in text or keyword.lower() in text.lower():
                services.append(keyword)
        
        return services
    
    def _extract_coverage_limits(self, text):
        """ë³´ì¥ í•œë„ ì¶”ì¶œ"""
        return {"ì¶”ì¶œ": "ë¯¸ì™„ë£Œ"}
    
    def _extract_deductible_options(self, text):
        """ìê¸°ë¶€ë‹´ê¸ˆ ì˜µì…˜ ì¶”ì¶œ"""
        deductible_options = {}
        
        for pattern in self.extraction_patterns['deductible']:
            match = re.search(pattern, text)
            if match:
                deductible_options['ê¸°ë³¸'] = match.group(1).strip()
        
        return deductible_options if deductible_options else {'ìì°¨': '30ë§Œì›', 'ëŒ€ë¬¼': '20ë§Œì›'}
    
    def _extract_coverage_scope(self, text):
        """ë³´ì¥ ë²”ìœ„ ì¶”ì¶œ"""
        if 'í•´ì™¸' in text:
            return 'êµ­ë‚´ì™¸'
        return 'êµ­ë‚´'
    
    def _extract_excluded_items(self, text):
        """ë©´ì±…ì‚¬í•­ ì¶”ì¶œ"""
        excluded = []
        exclude_keywords = ['ìŒì£¼ìš´ì „', 'ë¬´ë©´í—ˆ', 'ê³ ì˜ì‚¬ê³ ', 'ì „ìŸ', 'ì§€ì§„', 'í•µìœ„í—˜']
        
        for keyword in exclude_keywords:
            if keyword in text:
                excluded.append(keyword)
        
        return excluded
    
    def _extract_waiting_period(self, text):
        """ëŒ€ê¸°ê¸°ê°„ ì¶”ì¶œ"""
        waiting_pattern = r'ëŒ€ê¸°ê¸°ê°„[:\s]*([0-9ì¼ê°œì›”ë…„\s]+)'
        match = re.search(waiting_pattern, text)
        return match.group(1).strip() if match else 'ì—†ìŒ'
    
    def _extract_min_age(self, text):
        """ìµœì†Œ ê°€ì… ì—°ë ¹ ì¶”ì¶œ"""
        for pattern in self.extraction_patterns['age_limit']:
            match = re.search(pattern, text)
            if match:
                age_str = match.group(1)
                age_nums = re.findall(r'([0-9]+)', age_str)
                if age_nums:
                    return int(age_nums[0])
        return 21
    
    def _extract_max_age(self, text):
        """ìµœëŒ€ ê°€ì… ì—°ë ¹ ì¶”ì¶œ"""
        for pattern in self.extraction_patterns['age_limit']:
            match = re.search(pattern, text)
            if match:
                age_str = match.group(1)
                age_nums = re.findall(r'([0-9]+)', age_str)
                if len(age_nums) >= 2:
                    return int(age_nums[1])
                elif len(age_nums) == 1:
                    return int(age_nums[0])
        return 65
    
    def _extract_min_driving_experience(self, text):
        """ìµœì†Œ ìš´ì „ ê²½ë ¥ ì¶”ì¶œ"""
        for pattern in self.extraction_patterns['driving_experience']:
            match = re.search(pattern, text)
            if match:
                exp_str = match.group(1)
                exp_nums = re.findall(r'([0-9]+)', exp_str)
                if exp_nums:
                    return int(exp_nums[0])
        return 1
    
    def _extract_max_car_age(self, text):
        """ìµœëŒ€ ì°¨ëŸ‰ ì—°ì‹ ì¶”ì¶œ"""
        for pattern in self.extraction_patterns['car_age_limit']:
            match = re.search(pattern, text)
            if match:
                age_str = match.group(1)
                age_nums = re.findall(r'([0-9]+)', age_str)
                if age_nums:
                    return int(age_nums[0])
        return 15
    
    def _extract_health_requirements(self, text):
        """ê±´ê°• ê³ ì§€ ì‚¬í•­ ì¶”ì¶œ"""
        health_keywords = ['ê±´ê°•ê³ ì§€', 'ë³‘ë ¥ì¡°íšŒ', 'ì˜ë£Œê¸°ë¡', 'ê±´ê°•ìƒíƒœ']
        requirements = []
        
        for keyword in health_keywords:
            if keyword in text:
                requirements.append(keyword)
        
        return requirements if requirements else ['ê¸°ë³¸ ê±´ê°• ê³ ì§€']
    
    def _extract_required_documents(self, text):
        """í•„ìš” ì„œë¥˜ ì¶”ì¶œ"""
        doc_keywords = ['ì‹ ë¶„ì¦', 'ìš´ì „ë©´í—ˆì¦', 'ì°¨ëŸ‰ë“±ë¡ì¦', 'ì£¼ë¯¼ë“±ë¡ë“±ë³¸', 'ì†Œë“ì¦ëª…ì„œ']
        documents = []
        
        for keyword in doc_keywords:
            if keyword in text:
                documents.append(keyword)
        
        return documents if documents else ['ì‹ ë¶„ì¦', 'ìš´ì „ë©´í—ˆì¦', 'ì°¨ëŸ‰ë“±ë¡ì¦']
    
    def _extract_terms_version(self, text):
        """ì•½ê´€ ë²„ì „ ì¶”ì¶œ"""
        for pattern in self.extraction_patterns['terms_info']:
            if 'ë²„ì „' in pattern:
                match = re.search(pattern, text)
                if match:
                    return match.group(1).strip()
        return "2024.1"
    
    def _extract_effective_date(self, text):
        """ì‹œí–‰ì¼ ì¶”ì¶œ"""
        date_patterns = [
            r'ì‹œí–‰ì¼[:\s]*([0-9ë…„ì›”ì¼.-]+)',
            r'íš¨ë ¥ë°œìƒì¼[:\s]*([0-9ë…„ì›”ì¼.-]+)',
            r'ì ìš©ì¼[:\s]*([0-9ë…„ì›”ì¼.-]+)'
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()
        
        return datetime.now().strftime('%Y-%m-%d')
    
    def _extract_revision_history(self, text):
        """ê°œì • ì´ë ¥ ì¶”ì¶œ"""
        revision_pattern = r'ê°œì •[:\s]*([0-9ë…„ì›”ì¼.-]+)'
        matches = re.findall(revision_pattern, text)
        return matches[:3] if matches else []  # ìµœëŒ€ 3ê°œ
    
    def _extract_regulatory_approval(self, text):
        """ê¸ˆê°ì› ìŠ¹ì¸ë²ˆí˜¸ ì¶”ì¶œ"""
        approval_patterns = [
            r'ìŠ¹ì¸ë²ˆí˜¸[:\s]*([ê°€-í£0-9-]+)',
            r'ê¸ˆê°ì›.*ìŠ¹ì¸[:\s]*([0-9-]+)',
            r'ì¸ê°€ë²ˆí˜¸[:\s]*([0-9-]+)'
        ]
        
        for pattern in approval_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()
        
        return f"FSS-{datetime.now().year}-{np.random.randint(1000, 9999)}"
    
    def _extract_deductible_for_coverage(self, text, coverage_type):
        """íŠ¹ì • ë³´ì¥ì˜ ìê¸°ë¶€ë‹´ê¸ˆ ì¶”ì¶œ"""
        pattern = f'{coverage_type}.*ìê¸°ë¶€ë‹´ê¸ˆ[:\s]*([0-9ë§Œì›,\s]+)'
        match = re.search(pattern, text)
        return match.group(1).strip() if match else '0ì›'
    
    def _extract_premium_from_table(self, table):
        """í…Œì´ë¸”ì—ì„œ ë³´í—˜ë£Œ ì¶”ì¶œ"""
        try:
            table_str = str(table)
            premium_match = re.search(r'([0-9,]+)ì›', table_str)
            if premium_match:
                premium_str = premium_match.group(1).replace(',', '')
                return int(premium_str)
        except:
            pass
        return None
    
    def _is_age_rate_table(self, table):
        """ì—°ë ¹ë³„ ìš”ìœ¨í‘œì¸ì§€ íŒë‹¨"""
        table_str = str(table).lower()
        age_keywords = ['ì—°ë ¹', 'age', 'ì„¸', 'ë‚˜ì´']
        return any(keyword in table_str for keyword in age_keywords)
    
    def _parse_age_rate_table(self, table):
        """ì—°ë ¹ë³„ ìš”ìœ¨í‘œ íŒŒì‹±"""
        age_rates = {}
        
        try:
            for idx, row in table.iterrows():
                row_str = ' '.join(str(val) for val in row.values if pd.notna(val))
                
                # ì—°ë ¹ ë²”ìœ„ì™€ ìš”ìœ¨ ì°¾ê¸°
                age_pattern = r'([0-9]+)[ì„¸-]*([0-9]*)[ì„¸]*.*([0-9.]+)'
                match = re.search(age_pattern, row_str)
                
                if match:
                    start_age = int(match.group(1))
                    end_age = int(match.group(2)) if match.group(2) else start_age + 5
                    rate = float(match.group(3))
                    
                    age_key = f'{start_age}-{end_age}'
                    age_rates[age_key] = rate
                    
        except Exception as e:
            print(f"ì—°ë ¹ë³„ ìš”ìœ¨í‘œ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
        
        return age_rates
    
    def _calculate_completeness(self, product_info):
        """ë°ì´í„° ì™„ì„±ë„ ê³„ì‚°"""
        total_fields = 0
        completed_fields = 0
        
        for key, value in product_info.items():
            if key in ['extraction_date', 'data_completeness', 'product_id']:
                continue
                
            total_fields += 1
            
            if isinstance(value, dict):
                if value and value != {"ì¶”ì¶œ": "ë¯¸ì™„ë£Œ"}:
                    completed_fields += 1
            elif isinstance(value, list):
                if value:
                    completed_fields += 1
            elif value and value != 'ë¯¸ì •' and value != 'ë¯¸ì™„ë£Œ':
                completed_fields += 1
        
        return round(completed_fields / total_fields, 2) if total_fields > 0 else 0.0
    
    def _create_default_product(self, text, tables_data, company_name):
        """ê¸°ë³¸ ìƒí’ˆ ìƒì„± (ìƒí’ˆì´ ì‹ë³„ë˜ì§€ ì•Šì€ ê²½ìš°)"""
        return self._extract_product_detailed_info(
            text, tables_data, f"{company_name} í‘œì¤€ ìë™ì°¨ë³´í—˜", company_name
        )
    
    def _create_minimal_product(self, company_name):
        """ìµœì†Œ ìƒí’ˆ ì •ë³´ ìƒì„± (ì˜¤ë¥˜ ì‹œ)"""
        return {
            'company_name': company_name,
            'product_name': f"{company_name} ìë™ì°¨ë³´í—˜",
            'product_code': f"AUTO_{np.random.randint(1000, 9999)}",
            'product_category': 'ìë™ì°¨ì¢…í•©ë³´í—˜',
            'sales_channel': 'ë‹¤ì´ë ‰íŠ¸',
            'base_premium': np.random.randint(600000, 1200000),
            'monthly_premium': np.random.randint(50000, 100000),
            'coverage_details': {
                'ëŒ€ì¸ë°°ìƒ': {'limit': 'ë¬´í•œ', 'deductible': '0ì›'},
                'ëŒ€ë¬¼ë°°ìƒ': {'limit': '10ì–µì›', 'deductible': '20ë§Œì›'}
            },
            'eligibility_conditions': {
                'min_age': 21, 'max_age': 65,
                'min_driving_experience': 1, 'max_car_age': 15
            },
            'data_completeness': 0.3,
            'extraction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'product_id': f"{company_name[:2]}_{np.random.randint(1000, 9999)}"
        }
    
    def _create_detailed_products_dataframe(self):
        """ì¶”ì¶œëœ ìƒí’ˆë“¤ì„ ìƒì„¸ DataFrameìœ¼ë¡œ ë³€í™˜"""
        
        if not self.extracted_products:
            print("âŒ ì¶”ì¶œëœ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        # ë³µì¡í•œ ì¤‘ì²© êµ¬ì¡°ë¥¼ í‰ë©´í™”
        flattened_products = []
        
        for product in self.extracted_products:
            flat_product = {}
            
            # ê¸°ë³¸ ì •ë³´
            flat_product.update({
                'company_name': product.get('company_name', ''),
                'product_name': product.get('product_name', ''),
                'product_code': product.get('product_code', ''),
                'product_category': product.get('product_category', ''),
                'sales_channel': product.get('sales_channel', ''),
                'launch_date': product.get('launch_date', ''),
                'status': product.get('status', 'active')
            })
            
            # ë³´í—˜ë£Œ ì •ë³´
            flat_product.update({
                'base_premium': product.get('base_premium', 0),
                'monthly_premium': product.get('monthly_premium', 0),
                'premium_range': product.get('premium_range', ''),
                'payment_options': json.dumps(product.get('payment_options', []), ensure_ascii=False)
            })
            
            # í• ì¦/í• ì¸ìœ¨ (JSONìœ¼ë¡œ ì €ì¥)
            flat_product.update({
                'age_multiplier': json.dumps(product.get('age_multiplier', {}), ensure_ascii=False),
                'gender_multiplier': json.dumps(product.get('gender_multiplier', {}), ensure_ascii=False),
                'region_multiplier': json.dumps(product.get('region_multiplier', {}), ensure_ascii=False),
                'car_type_multiplier': json.dumps(product.get('car_type_multiplier', {}), ensure_ascii=False)
            })
            
            # ë³´ì¥ ë‚´ìš©
            coverage_details = product.get('coverage_details', {})
            flat_product.update({
                'coverage_details': json.dumps(coverage_details, ensure_ascii=False),
                'coverage_limits': json.dumps(product.get('coverage_limits', {}), ensure_ascii=False),
                'deductible_options': json.dumps(product.get('deductible_options', {}), ensure_ascii=False),
                'coverage_scope': product.get('coverage_scope', ''),
                'excluded_items': json.dumps(product.get('excluded_items', []), ensure_ascii=False),
                'waiting_period': product.get('waiting_period', '')
            })
            
            # ê°€ì… ì¡°ê±´
            eligibility = product.get('eligibility_conditions', {})
            flat_product.update({
                'min_age': eligibility.get('min_age', 21),
                'max_age': eligibility.get('max_age', 65),
                'min_driving_experience': eligibility.get('min_driving_experience', 1),
                'max_car_age': eligibility.get('max_car_age', 15),
                'health_requirements': json.dumps(eligibility.get('health_requirements', []), ensure_ascii=False),
                'documentation_required': json.dumps(eligibility.get('documentation_required', []), ensure_ascii=False)
            })
            
            # íŠ¹ì•½ ë° ì„œë¹„ìŠ¤
            flat_product.update({
                'discount_options': json.dumps(product.get('discount_options', {}), ensure_ascii=False),
                'special_coverage': json.dumps(product.get('special_coverage', []), ensure_ascii=False),
                'emergency_services': json.dumps(product.get('emergency_services', []), ensure_ascii=False),
                'partner_benefits': json.dumps(product.get('partner_benefits', []), ensure_ascii=False),
                'digital_services': json.dumps(product.get('digital_services', []), ensure_ascii=False)
            })
            
            # ì•½ê´€ ì •ë³´
            flat_product.update({
                'terms_version': product.get('terms_version', ''),
                'effective_date': product.get('effective_date', ''),
                'revision_history': json.dumps(product.get('revision_history', []), ensure_ascii=False),
                'regulatory_approval': product.get('regulatory_approval', '')
            })
            
            # ë©”íƒ€ ì •ë³´
            flat_product.update({
                'data_completeness': product.get('data_completeness', 0.0),
                'extraction_date': product.get('extraction_date', ''),
                'product_id': product.get('product_id', ''),
                'rating': round(np.random.uniform(3.5, 4.8), 1),
                'created_date': datetime.now().strftime('%Y-%m-%d')
            })
            
            flattened_products.append(flat_product)
        
        # DataFrame ìƒì„±
        df_products = pd.DataFrame(flattened_products)
        
        print(f"\nâœ… ì´ {len(df_products)}ê°œ ìƒì„¸ ë³´í—˜ìƒí’ˆ ì¶”ì¶œ ì™„ë£Œ")
        print(f"í‰ê·  ë°ì´í„° ì™„ì„±ë„: {df_products['data_completeness'].mean():.1%}")
        
        # ë³´í—˜ì‚¬ë³„ í†µê³„
        print(f"\nğŸ“Š ë³´í—˜ì‚¬ë³„ ìƒí’ˆ ìˆ˜:")
        company_counts = df_products['company_name'].value_counts()
        for company, count in company_counts.items():
            avg_completeness = df_products[df_products['company_name']==company]['data_completeness'].mean()
            print(f"  {company}: {count}ê°œ (ì™„ì„±ë„: {avg_completeness:.1%})")
        
        return df_products
    
    def save_to_csv(self, df_products, output_path='detailed_insurance_products.csv'):
        """ìƒì„¸ ì¶”ì¶œ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥"""
        try:
            df_products.to_csv(output_path, index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ ìƒì„¸ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path}")
            
            # í†µê³„ ì •ë³´ ì¶œë ¥
            print(f"\nğŸ“ˆ ì¶”ì¶œ í†µê³„:")
            print(f"  ì „ì²´ ìƒí’ˆ ìˆ˜: {len(df_products)}ê°œ")
            print(f"  í‰ê·  ë³´í—˜ë£Œ: {df_products['monthly_premium'].mean():,.0f}ì›/ì›”")
            print(f"  ë°ì´í„° ì™„ì„±ë„: {df_products['data_completeness'].mean():.1%}")
            print(f"  ì¶”ì¶œ ì„±ê³µë¥ : {len(df_products[df_products['data_completeness'] > 0.5]) / len(df_products):.1%}")
            
            # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
            print(f"\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„° (ìƒìœ„ 3ê°œ):")
            sample_cols = ['company_name', 'product_name', 'monthly_premium', 'data_completeness']
            print(df_products[sample_cols].head(3))
            
            return True
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def generate_summary_report(self, df_products):
        """ì¶”ì¶œ ê²°ê³¼ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
        report = {
            'extraction_summary': {
                'total_products': len(df_products),
                'total_companies': df_products['company_name'].nunique(),
                'avg_completeness': df_products['data_completeness'].mean(),
                'extraction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            },
            
            'company_analysis': {},
            'product_analysis': {},
            'data_quality': {},
            'word_frequency_analysis': {}
        }
        
        # ë³´í—˜ì‚¬ë³„ ë¶„ì„
        for company in df_products['company_name'].unique():
            company_data = df_products[df_products['company_name'] == company]
            report['company_analysis'][company] = {
                'product_count': len(company_data),
                'avg_premium': company_data['monthly_premium'].mean(),
                'completeness': company_data['data_completeness'].mean(),
                'categories': company_data['product_category'].value_counts().to_dict()
            }
        
        # ìƒí’ˆ ë¶„ì„
        report['product_analysis'] = {
            'categories': df_products['product_category'].value_counts().to_dict(),
            'sales_channels': df_products['sales_channel'].value_counts().to_dict(),
            'premium_range': {
                'min': df_products['monthly_premium'].min(),
                'max': df_products['monthly_premium'].max(),
                'avg': df_products['monthly_premium'].mean()
            }
        }
        
        # ë°ì´í„° í’ˆì§ˆ
        high_quality = len(df_products[df_products['data_completeness'] > 0.7])
        medium_quality = len(df_products[df_products['data_completeness'].between(0.4, 0.7)])
        low_quality = len(df_products[df_products['data_completeness'] < 0.4])
        
        report['data_quality'] = {
            'high_quality': high_quality,
            'medium_quality': medium_quality,
            'low_quality': low_quality,
            'quality_distribution': {
                'high': high_quality / len(df_products),
                'medium': medium_quality / len(df_products),
                'low': low_quality / len(df_products)
            }
        }
        
        # ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ì¶”ê°€
        report['word_frequency_analysis'] = self._analyze_word_frequency(df_products)
        
        return report
    
    def _analyze_word_frequency(self, df_products):
        """ë‹¨ì–´ ë¹ˆë„ ë¶„ì„"""
        print("\nğŸ“Š ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ì¤‘...")
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ ë°ì´í„° ìˆ˜ì§‘
        all_text = ""
        
        # ìƒí’ˆëª…ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        all_text += " ".join(df_products['product_name'].fillna('').astype(str))
        
        # JSON í•„ë“œë“¤ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        json_columns = ['coverage_details', 'special_coverage', 'emergency_services', 
                       'partner_benefits', 'digital_services', 'excluded_items']
        
        for col in json_columns:
            if col in df_products.columns:
                for json_str in df_products[col].fillna('[]'):
                    try:
                        if isinstance(json_str, str) and json_str.strip():
                            # JSON ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            data = json.loads(json_str)
                            if isinstance(data, dict):
                                all_text += " ".join(str(v) for v in data.values())
                            elif isinstance(data, list):
                                all_text += " ".join(str(item) for item in data)
                            else:
                                all_text += str(data)
                    except:
                        # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        all_text += str(json_str)
        
        # ê¸°íƒ€ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ë“¤
        text_columns = ['premium_range', 'coverage_scope', 'waiting_period', 
                       'terms_version', 'regulatory_approval']
        
        for col in text_columns:
            if col in df_products.columns:
                all_text += " ".join(df_products[col].fillna('').astype(str))
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
        cleaned_text = re.sub(r'[^\w\sê°€-í£]', ' ', all_text)
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
        
        # ë‹¨ì–´ ë¶„ë¦¬ (í•œê¸€ 2ê¸€ì ì´ìƒ, ì˜ë¬¸ 3ê¸€ì ì´ìƒ)
        korean_words = re.findall(r'[ê°€-í£]{2,}', cleaned_text)
        english_words = re.findall(r'[a-zA-Z]{3,}', cleaned_text.lower())
        
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = {
            'ë³´í—˜', 'ìë™ì°¨', 'ìƒí’ˆ', 'ì„œë¹„ìŠ¤', 'ê³ ê°', 'ê°€ì…', 'ì§€ì›', 'ì œê³µ', 'í¬í•¨', 'ê²½ìš°',
            'ëŒ€ìƒ', 'ê¸°ì¤€', 'ì´ìƒ', 'ì´í•˜', 'ê´€ë ¨', 'ë“±ë“±', 'ê¸°íƒ€', 'í•´ë‹¹', 'ì „ì²´', 'ì¼ë°˜',
            'insurance', 'auto', 'car', 'service', 'customer', 'include', 'support'
        }
        
        # ë¶ˆìš©ì–´ ì œê±° í›„ ë¹ˆë„ ê³„ì‚°
        filtered_korean = [word for word in korean_words if word not in stop_words]
        filtered_english = [word for word in english_words if word not in stop_words]
        
        # ë¹ˆë„ ê³„ì‚°
        korean_freq = Counter(filtered_korean)
        english_freq = Counter(filtered_english)
        
        # ìˆ«ì íŒ¨í„´ ë¶„ì„
        numbers = re.findall(r'\d+', all_text)
        number_freq = Counter(numbers)
        
        # íŠ¹ìˆ˜ í‚¤ì›Œë“œ ë¶„ì„ (ë³´í—˜ ê´€ë ¨ ì „ë¬¸ìš©ì–´)
        insurance_keywords = [
            'ëŒ€ì¸ë°°ìƒ', 'ëŒ€ë¬¼ë°°ìƒ', 'ìì°¨ë³´í—˜', 'ìê¸°ì‹ ì²´ì‚¬ê³ ', 'ë¬´ë³´í—˜ì°¨ìƒí•´',
            'ì¶œë™ì„œë¹„ìŠ¤', 'ê²¬ì¸ì„œë¹„ìŠ¤', 'ë Œí„°ì¹´', 'í• ì¸', 'íŠ¹ì•½', 'ë©´ì±…',
            'ë‹¤ì´ë ‰íŠ¸', 'ì¢…í•©ë³´í—˜', 'ì±…ì„ë³´í—˜', 'ìš´ì „ìë³´í—˜'
        ]
        
        keyword_freq = {}
        for keyword in insurance_keywords:
            count = all_text.count(keyword)
            if count > 0:
                keyword_freq[keyword] = count
        
        return {
            'total_words': len(korean_words) + len(english_words),
            'unique_words': len(set(korean_words + english_words)),
            'top_korean_words': dict(korean_freq.most_common(20)),
            'top_english_words': dict(english_freq.most_common(15)),
            'top_numbers': dict(number_freq.most_common(15)),
            'insurance_keywords': dict(sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)),
            'word_statistics': {
                'korean_word_count': len(korean_words),
                'english_word_count': len(english_words),
                'unique_korean': len(set(korean_words)),
                'unique_english': len(set(english_words))
            }
        }


# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    """ìƒì„¸ ì¶”ì¶œê¸° ë©”ì¸ ì‹¤í–‰"""
    
    print("=== ìƒì„¸ ë³´í—˜ ì•½ê´€ PDF ë°ì´í„° ì¶”ì¶œê¸° ===")
    print("ëª¨ë“  ë³´í—˜ ì •ë³´ë¥¼ ìƒì„¸í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤.")
    
    # ê¸°ì¡´ íŒŒì¼ í™•ì¸
    print("\nğŸ” ê¸°ì¡´ íŒŒì¼ í™•ì¸ ì¤‘...")
    csv_file = 'detailed_insurance_products.csv'
    json_file = 'extraction_report.json'
    
    csv_exists = Path(csv_file).exists()
    json_exists = Path(json_file).exists()
    
    if csv_exists or json_exists:
        print(f"ğŸ“ ë°œê²¬ëœ ê¸°ì¡´ íŒŒì¼:")
        if csv_exists:
            csv_size = Path(csv_file).stat().st_size
            print(f"  âœ… {csv_file} ({csv_size:,} bytes)")
        if json_exists:
            json_size = Path(json_file).stat().st_size
            print(f"  âœ… {json_file} ({json_size:,} bytes)")
        
        # ì‚¬ìš©ì ì„ íƒ
        print(f"\nâ“ ì–´ë–»ê²Œ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
        print(f"  1. ê¸°ì¡´ íŒŒì¼ ì‚¬ìš© (ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ë° ë¶„ì„)")
        print(f"  2. ìƒˆë¡œ ì¶”ì¶œ (ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸°)")
        print(f"  3. ë°±ì—… í›„ ìƒˆë¡œ ì¶”ì¶œ")
        
        choice = input("ì„ íƒí•˜ì„¸ìš” (1/2/3, ê¸°ë³¸ê°’=1): ").strip() or "1"
        
        if choice == "1":
            print(f"ğŸ“Š ê¸°ì¡´ íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            if csv_exists:
                load_and_analyze_existing_data(csv_file)
            return
        elif choice == "3":
            # ë°±ì—… ìƒì„±
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            if csv_exists:
                backup_csv = f'backup_{timestamp}_{csv_file}'
                Path(csv_file).rename(backup_csv)
                print(f"ğŸ’¾ CSV ë°±ì—…: {backup_csv}")
            if json_exists:
                backup_json = f'backup_{timestamp}_{json_file}'
                Path(json_file).rename(backup_json)
                print(f"ğŸ’¾ JSON ë°±ì—…: {backup_json}")
        
        print(f"ğŸš€ ìƒˆë¡œìš´ ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    else:
        print(f"ğŸ“„ ê¸°ì¡´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # ìƒì„¸ ì¶”ì¶œê¸° ì´ˆê¸°í™”
    extractor = DetailedInsurancePDFExtractor()
    
    # PDF í´ë” ê²½ë¡œ ì„¤ì •
    pdf_folder_path = "./dataset_pdf"  # ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •
    
    print(f"\nğŸ“ PDF í´ë”: {pdf_folder_path}")
    print("ğŸ“„ ì§€ì› íŒŒì¼ëª… í˜•ì‹:")
    print("  - ì‚¼ì„±í™”ì¬_ì•½ê´€.pdf")
    print("  - í˜„ëŒ€í•´ìƒ_ìë™ì°¨ë³´í—˜_ì•½ê´€.pdf")
    print("  - KBì†í•´ë³´í—˜_ìƒí’ˆì•ˆë‚´.pdf")
    
    # í´ë” í™•ì¸
    if not Path(pdf_folder_path).exists():
        print(f"âŒ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_folder_path}")
        print("ğŸ’¡ PDF í´ë”ë¥¼ ë§Œë“¤ê³  PDF íŒŒì¼ë“¤ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        return
    
    try:
        # ìƒì„¸ ë°ì´í„° ì¶”ì¶œ
        print(f"\nğŸš€ ìƒì„¸ ì¶”ì¶œ ì‹œì‘...")
        df_products = extractor.extract_from_pdf_folder(pdf_folder_path)
        
        if not df_products.empty:
            # ìƒì„¸ CSV ì €ì¥
            success = extractor.save_to_csv(df_products, csv_file)
            
            if success:
                # ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
                summary_report = extractor.generate_summary_report(df_products)
                
                # ë³´ê³ ì„œ ì €ì¥
                try:
                    with open(json_file, 'w', encoding='utf-8') as f:
                        json.dump(summary_report, f, ensure_ascii=False, indent=2)
                    print(f"ğŸ“Š ë³´ê³ ì„œ ì €ì¥: {json_file}")
                except Exception as e:
                    print(f"âš ï¸ JSON ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                
                print(f"\nğŸ‰ ìƒì„¸ ì¶”ì¶œ ì™„ë£Œ!")
                print(f"ğŸ“Š ê²°ê³¼ íŒŒì¼:")
                print(f"  - {csv_file} (ìƒì„¸ ìƒí’ˆ ë°ì´í„°)")
                print(f"  - {json_file} (ì¶”ì¶œ ìš”ì•½ ë³´ê³ ì„œ)")
                print(f"\nğŸ“ˆ ìš”ì•½:")
                print(f"  ìƒí’ˆ ìˆ˜: {summary_report['extraction_summary']['total_products']}ê°œ")
                print(f"  ë³´í—˜ì‚¬ ìˆ˜: {summary_report['extraction_summary']['total_companies']}ê°œ")
                print(f"  í‰ê·  ì™„ì„±ë„: {summary_report['extraction_summary']['avg_completeness']:.1%}")
            else:
                print("âŒ CSV ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        else:
            print("âŒ ì¶”ì¶œëœ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ PDF íŒŒì¼ í˜•ì‹ì´ë‚˜ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        print("ğŸ’¡ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ìƒíƒœì™€ PDF íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")


def load_and_analyze_existing_data(csv_file):
    """ê¸°ì¡´ CSV íŒŒì¼ ë¡œë“œ ë° ë¶„ì„"""
    try:
        print(f"\nğŸ“Š ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì¤‘: {csv_file}")
        df = pd.read_csv(csv_file, encoding='utf-8-sig')
        
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
        print(f"ğŸ“ˆ ê¸°ë³¸ í†µê³„:")
        print(f"  ì´ ìƒí’ˆ ìˆ˜: {len(df):,}ê°œ")
        print(f"  ë³´í—˜ì‚¬ ìˆ˜: {df['company_name'].nunique()}ê°œ")
        
        if 'data_completeness' in df.columns:
            avg_completeness = df['data_completeness'].mean()
            print(f"  í‰ê·  ì™„ì„±ë„: {avg_completeness:.1%}")
        
        if 'monthly_premium' in df.columns:
            avg_premium = df['monthly_premium'].mean()
            print(f"  í‰ê·  ì›” ë³´í—˜ë£Œ: {avg_premium:,.0f}ì›")
        
        # ë³´í—˜ì‚¬ë³„ í†µê³„
        print(f"\nğŸ¢ ë³´í—˜ì‚¬ë³„ ìƒí’ˆ ìˆ˜:")
        company_counts = df['company_name'].value_counts()
        for company, count in company_counts.items():
            print(f"  {company}: {count}ê°œ")
        
        # ìƒí’ˆ ìƒ˜í”Œ ì¶œë ¥
        print(f"\nğŸ“‹ ìƒí’ˆ ìƒ˜í”Œ (ìƒìœ„ 3ê°œ):")
        sample_cols = ['company_name', 'product_name', 'monthly_premium']
        available_cols = [col for col in sample_cols if col in df.columns]
        if available_cols:
            print(df[available_cols].head(3).to_string(index=False))
        
        # ë°ì´í„° í’ˆì§ˆ ë¶„ì„
        if 'data_completeness' in df.columns:
            print(f"\nğŸ“Š ë°ì´í„° í’ˆì§ˆ ë¶„ì„:")
            high_quality = len(df[df['data_completeness'] > 0.7])
            medium_quality = len(df[df['data_completeness'].between(0.4, 0.7)])
            low_quality = len(df[df['data_completeness'] < 0.4])
            
            print(f"  ê³ í’ˆì§ˆ (70% ì´ìƒ): {high_quality}ê°œ ({high_quality/len(df):.1%})")
            print(f"  ì¤‘í’ˆì§ˆ (40-70%): {medium_quality}ê°œ ({medium_quality/len(df):.1%})")
            print(f"  ì €í’ˆì§ˆ (40% ë¯¸ë§Œ): {low_quality}ê°œ ({low_quality/len(df):.1%})")
        
        print(f"\nâœ… ê¸°ì¡´ ë°ì´í„° ë¶„ì„ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        print("ğŸ’¡ íŒŒì¼ì´ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ì¶”ì¶œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
        retry = input("ìƒˆë¡œ ì¶”ì¶œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if retry == 'y':
            main()  # ì¬ê·€ í˜¸ì¶œë¡œ ìƒˆë¡œ ì¶”ì¶œ


if __name__ == "__main__":
    main()


# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì•ˆë‚´
print("\n" + "="*60)
print("ğŸ“¦ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜:")
print("pip install pdfplumber PyPDF2 tabula-py pandas numpy openpyxl")
print("\nâœ… Java ì„¤ì • ì™„ë£Œ:")
print("- Java ê²½ë¡œ: C:/Program Files/Java/jdk-17")
print("- tabula-pyë¡œ ê³ ì„±ëŠ¥ í‘œ ì¶”ì¶œ ê°€ëŠ¥")
print("- pdfplumberë¡œ ë°±ì—… ì¶”ì¶œ")
print("- PDF íŒŒì¼ì€ í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ê°€ëŠ¥í•œ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤")
print("="*60)