import os
import fitz  # PyMuPDF
import pytesseract
from PIL import Image
import camelot
import re
import json
from pathlib import Path
from datetime import datetime
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from tqdm import tqdm  # ì§„í–‰ë¥  í‘œì‹œ + ETA

PDF_FOLDER = "./dataset_pdf"
OUTPUT_JSON = "./pdf_extracted.json"
CHUNK_SIZE = 500
FAISS_INDEX = "./faiss_index.index"
EMBED_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
embed_model = SentenceTransformer(EMBED_MODEL)

# ========== ìœ í‹¸ ==========
def extract_text_pymupdf(pdf_path):
    doc = fitz.open(pdf_path)
    text_pages = []
    for page_num, page in enumerate(doc, start=1):
        text = page.get_text("text").strip()
        text_pages.append({"page": page_num, "text": text})
    return text_pages

def ocr_page(pdf_path, page_num):
    doc = fitz.open(pdf_path)
    page = doc[page_num-1]
    pix = page.get_pixmap(dpi=200)
    img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
    text = pytesseract.image_to_string(img, lang="kor+eng").strip()
    return text

def extract_tables(pdf_path):
    tables_data = []
    try:
        tables = camelot.read_pdf(pdf_path, pages="all", flavor="lattice")
        for t in tables:
            data = {
                "page": t.page,
                "columns": t.df.iloc[0].tolist(),
                "rows": t.df.iloc[1:].values.tolist()
            }
            tables_data.append(data)
    except Exception as e:
        print(f"[WARN] í‘œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    return tables_data

def chunk_text(text, max_len=CHUNK_SIZE):
    sentences = re.split(r'(?<=[.!?])\s+', text)
    chunks, current = [], ""
    for s in sentences:
        if len(current) + len(s) < max_len:
            current += " " + s
        else:
            chunks.append(current.strip())
            current = s
    if current:
        chunks.append(current.strip())
    return chunks

# ========== FAISS ë¡œì»¬ ë²¡í„°DB ==========
def init_faiss(dimension=384):
    if os.path.exists(FAISS_INDEX):
        index = faiss.read_index(FAISS_INDEX)
    else:
        index = faiss.IndexFlatL2(dimension)
    return index

def save_faiss(index):
    faiss.write_index(index, FAISS_INDEX)

# ========== ë©”ì¸ íŒŒì´í”„ë¼ì¸ ==========
def process_pdf(pdf_path, faiss_index):
    file_name = Path(pdf_path).stem
    print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {file_name}")

    results = []
    text_pages = extract_text_pymupdf(pdf_path)
    total_pages = len(text_pages)
    ocr_count = 0  # âœ… OCR ì ìš©ëœ í˜ì´ì§€ ì¹´ìš´í„°

    # tqdm ì§„í–‰ë¥  ë°” + ETA í‘œì‹œ
    for p in tqdm(text_pages, desc=f"ğŸ” OCR ì§„í–‰ ({file_name})", unit="page"):
        page_num, text = p["page"], p["text"]

        # OCR ì ìš© ì¡°ê±´
        if len(text) < 30:
            ocr_count += 1
            print(f"   â¡ OCR ì ìš©: {page_num}p ({ocr_count}/{total_pages} OCR ì™„ë£Œ)")
            text = ocr_page(pdf_path, page_num)

        results.append({
            "type": "text",
            "file_name": file_name,
            "page": page_num,
            "content": text,
            "extracted_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        })

    print(f"ğŸ“Š {file_name}: OCR ì ìš©ëœ í˜ì´ì§€ {ocr_count}/{total_pages}\n")

    # í‘œ ì¶”ì¶œ
    tables = extract_tables(pdf_path)
    for t in tables:
        results.append({
            "type": "table",
            "file_name": file_name,
            "page": t["page"],
            "columns": t["columns"],
            "rows": t["rows"],
            "extracted_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        })

    # í…ìŠ¤íŠ¸ ì„ë² ë”© â†’ FAISS ì €ì¥
    for item in results:
        if item["type"] == "text" and item["content"]:
            chunks = chunk_text(item["content"])
            vectors = embed_model.encode(chunks, convert_to_numpy=True)
            faiss_index.add(vectors)

    return results

def main():
    faiss_index = init_faiss()
    pdf_files = list(Path(PDF_FOLDER).glob("*.pdf"))
    if not pdf_files:
        print("âŒ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    all_data = []
    print(f"\nğŸ”„ ì´ {len(pdf_files)}ê°œ PDF ì²˜ë¦¬ ì‹œì‘...\n")

    for i, pdf in enumerate(pdf_files, start=1):
        print(f"\n[{i}/{len(pdf_files)}] {pdf.name} ì²˜ë¦¬ ì¤‘...")
        extracted = process_pdf(pdf, faiss_index)
        all_data.extend(extracted)

    # JSON ì €ì¥
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    save_faiss(faiss_index)

    # âœ… ì „ì²´ ì™„ë£Œ ì•Œë¦¼
    print(f"\nâœ… ëª¨ë“  PDF ì²˜ë¦¬ ë° ì„ë² ë”© ì™„ë£Œ!")
    print(f"ğŸ’¾ JSON ì €ì¥: {OUTPUT_JSON}")
    print(f"ğŸ’¾ FAISS ë²¡í„°DB ì €ì¥: {FAISS_INDEX}")
    print("ğŸ”” ì•Œë¦¼: ì „ì²´ ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ëë‚¬ìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
